#!/usr/bin/env python3
"""
Judge Severity Async Complete Flow Predicates
Generated by Graph29 Sequential Prompts Framework
Combines all flows: init, retrieve, api call, parser (exactly like dxGPT)
"""

import sys
import os
from pathlib import Path

# Add the parent directory to sys.path to import the base class
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / 'generators'))

from generators.predicate_graph import PredicateGraph

class JudgeSeverityAsyncCompleteGraph(PredicateGraph):
    def __init__(self):
        super().__init__("judge_severity_async_complete")
    
    def get_node_categories(self):
        """Return node category mapping for color assignment - exactly like dxGPT"""
        return {
            # Application Components (Green) - Main Components
            "run_judge_severity_async.py<br/>(Command-line entry)": "application_components",
            "judge_severity_async.py<br/>(Main async workflow)": "application_components",
            "models/judge_severity_models.py": "application_components",
            "prompts/judge_severity_prompts.py": "application_components",
            "parsers/judge_severity_parser.py": "application_components",
            "serialization/judge_severity_pydantic_models.py": "application_components",
            "queries/severity_queries.py<br/>(Database operations)": "application_components",
            
            # Core Functions (Dark Green)
            "set_settings()": "core_functions",
            "retrieve_and_make_prompts()": "core_functions",
            "process_results()": "core_functions",
            
            # Framework Components (Blue)
            "AsyncModelHandler": "framework_components",
            "process_all_batches()": "framework_components",
            "PromptBuilder": "framework_components",
            
            # Database Tables (Purple)
            "CasesBench<br/>(Input cases)": "database_tables",
            "Models<br/>(LLM configs)": "database_tables",
            "Prompts<br/>(Templates)": "database_tables",
            "GoldenDiagnoses<br/>(Ground truth)": "database_tables",
            "DifferentialDiagnosis2Rank<br/>(Ranked diagnoses)": "database_tables",
            "DifferentialDiagnosis2SeverityAssessment<br/>(Severity assessments)": "database_tables",
            
            # Database Functions (Red)
            "insert_or_fetch_model()<br/>insert_or_fetch_prompt()": "database_functions",
            "get_cases()<br/>get_case_to_golden_diagnosis_mapping()<br/>get_model_names_from_differential_diagnosis()<br/>get_ranks_for_hospital_and_model_id()<br/>get_model_id_from_name()<br/>create_nested_diagnosis_dict()": "database_functions",
            "add_severity_results_to_db()<br/>in severity_queries": "database_functions",
            
            # Selected Components (Yellow-Brown)
            "Selected Severity Judge Prompt<br/>(e.g., StandardSeverityJudgePrompt)": "selected_components",
            "Selected Severity Judge Model<br/>(e.g., LlamaThreeEightBConfig)": "selected_components",
            "Severity Assessment Parser<br/>(e.g., parse_judged_severity)": "selected_components",
            
            # Data Flow Elements (Light Gray)
            "1. inits configuration": "data_flow_elements",
            "model_id": "data_flow_elements",
            "prompt_id": "data_flow_elements",
            "case_bench_id": "data_flow_elements",
            "case_text": "data_flow_elements",
            "Case Model<br/>(Pydantic Schema)": "data_flow_elements",
            "(Judgment Cases Pydantic Schema)": "data_flow_elements",
            "List of Clinical Cases<br/>(Pydantic models)": "data_flow_elements",
            "List of Severity Assessments<br/>(plain text)": "data_flow_elements",
            "List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)": "data_flow_elements",
            "Raw Severity Assessment<br/>(JSON response)": "data_flow_elements",
            "differential_diagnosis_severity_assessment_id<br/>(Integer)": "data_flow_elements",
            "cases_bench_id<br/>(Integer)": "data_flow_elements",
            "rank_id<br/>(Integer)": "data_flow_elements",
            
            # Command Arguments (Yellow)
            "--model_alias": "command_arguments",
            "--prompt_alias": "command_arguments",
            "--differential_diagnosis_model<br/>--test_name<br/>--max_diagnoses": "command_arguments",
            "--batch_size<br/>--rpm_limit<br/>--min_batch_interval": "command_arguments",
        }
    
    def add_complete_flow(self):
        """Add complete flow predicates exactly like dxGPT structure"""
        
        triplets = [
            # --- EXECUTION FLOW (exactly like dxGPT) ---
            
            # Main execution flow
            ("run_judge_severity_async.py<br/>(Command-line entry)", "calls", "judge_severity_async.py<br/>(Main async workflow)"),
            ("judge_severity_async.py<br/>(Main async workflow)", "calls", "set_settings()"),
            ("judge_severity_async.py<br/>(Main async workflow)", "calls", "retrieve_and_make_prompts()"),
            ("judge_severity_async.py<br/>(Main async workflow)", "calls", "process_results()"),
            
            # set_settings output
            ("set_settings()", "produces", "1. inits configuration"),
            
            # config connections
            ("1. inits configuration", "produces", "--model_alias"),
            ("1. inits configuration", "produces", "--prompt_alias"),
            
            # Other function connections
            ("(Judgment Cases Pydantic Schema)", "sent to", "process_all_batches()"),
            
            # Component connections
            
            # Modified/Added links per request - Prompt flow
            ("--prompt_alias", "sent to", "prompts/judge_severity_prompts.py"),
            ("prompts/judge_severity_prompts.py", "loads", "Selected Severity Judge Prompt<br/>(e.g., StandardSeverityJudgePrompt)"),
            ("PromptBuilder", "extends", "Selected Severity Judge Prompt<br/>(e.g., StandardSeverityJudgePrompt)"),
            ("Selected Severity Judge Prompt<br/>(e.g., StandardSeverityJudgePrompt)", "used by", "process_all_batches()"),
            
            # Modified/Added links per request - Model flow
            ("--model_alias", "sent to", "process_all_batches()"),
            ("AsyncModelHandler", "selects", "models/judge_severity_models.py"),
            ("models/judge_severity_models.py", "loads", "Selected Severity Judge Model<br/>(e.g., LlamaThreeEightBConfig)"),
            ("Selected Severity Judge Model<br/>(e.g., LlamaThreeEightBConfig)", "produces", "List of Severity Assessments<br/>(plain text)"),
            
            # Arguments connections
            ("--batch_size<br/>--rpm_limit<br/>--min_batch_interval", "configures", "process_all_batches()"),
            ("retrieve_and_make_prompts()", "uses", "--differential_diagnosis_model<br/>--test_name<br/>--max_diagnoses"),
            ("--differential_diagnosis_model<br/>--test_name<br/>--max_diagnoses", "sent to", "get_cases()<br/>get_case_to_golden_diagnosis_mapping()<br/>get_model_names_from_differential_diagnosis()<br/>get_ranks_for_hospital_and_model_id()<br/>get_model_id_from_name()<br/>create_nested_diagnosis_dict()"),
            
            # Database module connections
            ("--model_alias", "sent to", "insert_or_fetch_model()<br/>insert_or_fetch_prompt()"),
            ("--prompt_alias", "sent to", "insert_or_fetch_model()<br/>insert_or_fetch_prompt()"),
            ("insert_or_fetch_model()<br/>insert_or_fetch_prompt()", "queries", "Models<br/>(LLM configs)"),
            ("insert_or_fetch_model()<br/>insert_or_fetch_prompt()", "queries", "Prompts<br/>(Templates)"),
            ("Models<br/>(LLM configs)", "returns", "model_id"),
            ("Prompts<br/>(Templates)", "returns", "prompt_id"),
            
            # CasesBench connections
            ("CasesBench<br/>(Input cases)", "returns", "case_bench_id"),
            ("CasesBench<br/>(Input cases)", "returns", "case_text"),
            ("case_bench_id", "part of", "Case Model<br/>(Pydantic Schema)"),
            ("case_text", "part of", "Case Model<br/>(Pydantic Schema)"),
            ("model_id", "part of", "Case Model<br/>(Pydantic Schema)"),
            ("prompt_id", "part of", "Case Model<br/>(Pydantic Schema)"),
            ("Case Model<br/>(Pydantic Schema)", "generates", "List of Clinical Cases<br/>(Pydantic models)"),
            
            # --- DATA FLOW (exactly like dxGPT) ---
            
            # retrieve_and_make_prompts() flow
            ("get_cases()<br/>get_case_to_golden_diagnosis_mapping()<br/>get_model_names_from_differential_diagnosis()<br/>get_ranks_for_hospital_and_model_id()<br/>get_model_id_from_name()<br/>create_nested_diagnosis_dict()", "queries", "CasesBench<br/>(Input cases)"),
            ("get_cases()<br/>get_case_to_golden_diagnosis_mapping()<br/>get_model_names_from_differential_diagnosis()<br/>get_ranks_for_hospital_and_model_id()<br/>get_model_id_from_name()<br/>create_nested_diagnosis_dict()", "queries", "GoldenDiagnoses<br/>(Ground truth)"),
            ("get_cases()<br/>get_case_to_golden_diagnosis_mapping()<br/>get_model_names_from_differential_diagnosis()<br/>get_ranks_for_hospital_and_model_id()<br/>get_model_id_from_name()<br/>create_nested_diagnosis_dict()", "queries", "DifferentialDiagnosis2Rank<br/>(Ranked diagnoses)"),
            ("serialization/judge_severity_pydantic_models.py", "stored in", "Case Model<br/>(Pydantic Schema)"),
            ("Case Model<br/>(Pydantic Schema)", "generates", "(Judgment Cases Pydantic Schema)"),
            
            # process_all_batches() flow
            ("process_all_batches()", "calls", "AsyncModelHandler"),
            
            # process_results() flow
            ("process_results()", "uses", "parsers/judge_severity_parser.py"),
            ("parsers/judge_severity_parser.py", "contains", "Severity Assessment Parser<br/>(e.g., parse_judged_severity)"),
            
            # Parser Data Flow
            ("List of Severity Assessments<br/>(plain text)", "passed to", "Severity Assessment Parser<br/>(e.g., parse_judged_severity)"),
            ("Severity Assessment Parser<br/>(e.g., parse_judged_severity)", "produces", "Raw Severity Assessment<br/>(JSON response)"),
            
            ("List of Clinical Cases<br/>(Pydantic models)", "augmented to", "List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)"),
            ("Raw Severity Assessment<br/>(JSON response)", "stored as", "List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)"),
            ("List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)", "sent to DB via", "add_severity_results_to_db()<br/>in severity_queries"),
            
            ("Severity Assessment Parser<br/>(e.g., parse_judged_severity)", "extracts", "differential_diagnosis_severity_assessment_id<br/>(Integer)"),
            ("Severity Assessment Parser<br/>(e.g., parse_judged_severity)", "extracts", "cases_bench_id<br/>(Integer)"),
            ("Severity Assessment Parser<br/>(e.g., parse_judged_severity)", "extracts", "rank_id<br/>(Integer)"),
            
            # Database Insert Flow
            ("judge_severity_async.py<br/>(Main async workflow)", "uses", "queries/severity_queries.py<br/>(Database operations)"),
            ("queries/severity_queries.py<br/>(Database operations)", "calls", "add_severity_results_to_db()<br/>in severity_queries"),
            ("add_severity_results_to_db()<br/>in severity_queries", "writes to", "DifferentialDiagnosis2SeverityAssessment<br/>(Severity assessments)"),
            
            ("differential_diagnosis_severity_assessment_id<br/>(Integer)", "stored in", "List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)"),
            ("cases_bench_id<br/>(Integer)", "stored in", "List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)"),
            ("rank_id<br/>(Integer)", "stored in", "List of Judgment Cases w/ Severity Results<br/>(Pydantic Models)"),
        ]
        
        for source, predicate, target in triplets:
            self.add_edge(source, predicate, target)

if __name__ == "__main__":
    # Create and build the graph
    graph = JudgeSeverityAsyncCompleteGraph()
    graph.add_complete_flow()
    
    # Print summary
    print(f"Generated {graph.module_name} with {len(graph.nodes)} nodes and {len(graph.edges)} edges")
    
    # Optionally print all triplets for verification
    for source, predicate, target in graph.get_triplets():
        print(f"  ({source}, {predicate}, {target})") 